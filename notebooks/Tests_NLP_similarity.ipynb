{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u0bb11ME2Qt",
    "tags": []
   },
   "source": [
    "# Miscellaneous notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIwbKUrOF1SL",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data source and idea\n",
    "\n",
    "From the github of HANNA: https://github.com/dig-team/hanna-benchmark-asg, \n",
    "we can retrieve the file hanna_stories_annotation.csv, which contains, for 96 prompts, a story generated by a human and a story generated by 10 ASG systems, so 1056 stories in total. \n",
    "\n",
    "The idea is to try to reproduce, to some extent the results depicted in \"Of human criteria and automatic metrics: a benchmark of the evaluation of story generation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_wSEkt8GFTj"
   },
   "source": [
    "## Various metrics\n",
    "\n",
    "* From https://github.com/PierreColombo/nlg_eval_via_simi_measures: DepthScore, BaryScore, InfoLM\n",
    "* From https://github.com/neural-dialogue-metrics/BLEU: BLEU\n",
    "* From NLTK : BLEU\n",
    "* From https://github.com/neural-dialogue-metrics/rouge: ROUGE\n",
    "* From https://github.com/pltrdy/rouge : ROUGE (alternative implementation)\n",
    "* From https://github.com/bheinzerling/pyrouge  pyrouge : ROUGE (Rouge155)\n",
    "* : METEOR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gndQyXoPZWLr"
   },
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GErMBE8mKPOx"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9jfmEHxKBr4",
    "outputId": "7fe8d4a3-685b-479c-cbdb-0053fb86e18d"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install pyrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWXT9r9fRWmb",
    "outputId": "dd323959-f78a-46f2-feaa-c2d5356adf06"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/PierreColombo/nlg_eval_via_simi_measures.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/neural-dialogue-metrics/BLEU.git\n",
    "!pip install git+https://github.com/neural-dialogue-metrics/rouge.git\n",
    "!pip install git+https://github.com/pltrdy/rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p30RJOwsGD7N"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Oa-W9bhUnAi"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn\n",
    "import torch \n",
    "import transformers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb5H2WdfQzAM"
   },
   "outputs": [],
   "source": [
    "from nlg_eval_via_simi_measures.bary_score import BaryScoreMetric\n",
    "from nlg_eval_via_simi_measures.depth_score import DepthScoreMetric\n",
    "from nlg_eval_via_simi_measures.infolm import InfoLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjyPRpzhVnyX"
   },
   "source": [
    "# Testing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVK8xtYtVqFk",
    "outputId": "e45d5b89-104e-4a47-9606-3805dfb6425d"
   },
   "outputs": [],
   "source": [
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "# Baryscore\n",
    "\n",
    "def compute_bary(ref, hypothesis):\n",
    "  metric_call = BaryScoreMetric()\n",
    "  metric_call.prepare_idfs(ref, hypothesis)\n",
    "  final_preds = metric_call.evaluate_batch(ref, hypothesis)\n",
    "  print(final_preds)\n",
    "  print(\"=\"*25)\n",
    "\n",
    "compute_bary(ref, hypothesis)\n",
    "\n",
    "ref = ['I like my cakes very much']\n",
    "hypothesis = ['I like my cakes very much']\n",
    "\n",
    "compute_bary(ref, hypothesis)\n",
    "\n",
    "ref = ['I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much']\n",
    "\n",
    "compute_bary(ref, hypothesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPeCTkc9d3M6"
   },
   "source": [
    "*   W c'est pour distance de Wasserstein\n",
    "*   SD, c'est pour Sinkhorn Divergence, donc autre chose et a priori pas ce qui nous intéresse (pour différentes valeurs d'un paramètre)\n",
    "\n",
    "Donc, a priori, on prend juste \"baryscore_W\"\n",
    "\n",
    "* Il faut calculer les idf au niveau du corpus, mais d'après ce que je lis dans l'article, ca se fait séparemment pour chaque paire référence/candidat (en tout cas le contraire n'est pas clairement indiqué)\n",
    "\n",
    "Donc, à mon avis, même s'il y a une fonction \"evaluate_batch\", on doit faire les calculs individuellement pour chaque pair, et pas tout concaténer. Mais le calcul de l'idf doit se faire au niveau des **textes** complets que l'on compare. \n",
    "\n",
    "Plus la métrique est proche de 0, mieux c'est. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ft-H4fiVcTI8",
    "outputId": "5ac2343a-07d4-4a98-8756-fb056839e84e"
   },
   "outputs": [],
   "source": [
    "# DepthScore\n",
    "\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "metric_call = DepthScoreMetric()\n",
    "metric_call.prepare_idfs(ref, hypothesis)\n",
    "final_preds = metric_call.evaluate_batch(hypothesis,ref )\n",
    "print(\"DepthScore\")\n",
    "print(final_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dba27g1_hGFa"
   },
   "source": [
    "Calcul d'un embedding pour le candidat et la référence avec a single layer of Bert (comme Bertscore apparemment) puis calcule la divergence introduite dans \"a pseudo metric\", donc plus c'est proche de 0 et plus les deux phrases sont proches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iE5F3oygM5QV",
    "outputId": "0b571f2a-2849-46f9-f29a-90a203aa743f"
   },
   "outputs": [],
   "source": [
    "# Pour l'instant, ca fonctionne uniquement si on se met sur CPU (il manque surement un .todevice() quelque part)\n",
    "\n",
    "# InfoLM \n",
    "\n",
    "metric = InfoLM()\n",
    "\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "metric.prepare_idfs(ref, hypothesis)\n",
    "\n",
    "final_preds = metric.evaluate_batch(hypothesis, ref)#), \n",
    "                                    #idf_ref = metric.idf_dict_hyp,\n",
    "                                    #idf_hyps = metric.idf_dict_ref)\n",
    "                                    #idf_ref= idf_ref,\n",
    "                                    #idf_hyps= idf_hypot)\n",
    "\n",
    "#self.idf_dict_hyp, self.idf_dict_ref \n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2sbFBJ7iApZ"
   },
   "source": [
    "Encore une fois, il me semble qu'on calcule les idf à l'échelle d'une pair candidat (C) / référence (R). \n",
    "D'abord, séparemment pour C et R, le PMLM calcule une probabilité discrète par token en le masquant, puis on l'agrège en mettant des poids via les idf, puis on utilise une mesure de divergence pour estimer la distance entre les deux distributions de probabilité obtenues. \n",
    "\n",
    "Par défaut, on utilise Fisher Rao qui, d'après l'article, est pas mal et a l'avantage de ne pas inclure de paramètre à optimiser. \n",
    "\n",
    "On a 3 résultats, parce que l'algo permet d'utiliser des divergences, qui ne sont pas symétriques. Du coup le premier c'est Div(A,B), le second Div(B,A) et le 3ème la moyenne des deux. Mais vu qu'on utilise la DISTANCE de Fisher Rao (qui évidemment est une distance), elle est symétrique et osef"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
