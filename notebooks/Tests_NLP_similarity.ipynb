{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u0bb11ME2Qt",
    "tags": []
   },
   "source": [
    "# Miscellaneous notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIwbKUrOF1SL",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data source and idea\n",
    "\n",
    "From the github of HANNA: https://github.com/dig-team/hanna-benchmark-asg, \n",
    "we can retrieve the file hanna_stories_annotation.csv, which contains, for 96 prompts, a story generated by a human and a story generated by 10 ASG systems, so 1056 stories in total. \n",
    "\n",
    "The idea is to try to reproduce, to some extent the results depicted in \"Of human criteria and automatic metrics: a benchmark of the evaluation of story generation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_wSEkt8GFTj"
   },
   "source": [
    "## Various metrics\n",
    "\n",
    "* From https://github.com/PierreColombo/nlg_eval_via_simi_measures: DepthScore, BaryScore, InfoLM\n",
    "* From https://github.com/neural-dialogue-metrics/BLEU: BLEU\n",
    "* From NLTK : BLEU\n",
    "* From https://github.com/neural-dialogue-metrics/rouge: ROUGE\n",
    "* From https://github.com/pltrdy/rouge : ROUGE (alternative implementation)\n",
    "* From https://github.com/bheinzerling/pyrouge  pyrouge : ROUGE (Rouge155)\n",
    "* From NLTK : METEOR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gndQyXoPZWLr"
   },
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GErMBE8mKPOx"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9jfmEHxKBr4",
    "outputId": "7fe8d4a3-685b-479c-cbdb-0053fb86e18d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install py-rouge\n",
    "#https://github.com/Diego999/py-rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWXT9r9fRWmb",
    "outputId": "dd323959-f78a-46f2-feaa-c2d5356adf06",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/PierreColombo/nlg_eval_via_simi_measures.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/neural-dialogue-metrics/BLEU.git\n",
    "\n",
    "#those two have the same package name : rouge\n",
    "#!pip install git+https://github.com/neural-dialogue-metrics/rouge.git\n",
    "#!pip install git+https://github.com/pltrdy/rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p30RJOwsGD7N"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Oa-W9bhUnAi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn\n",
    "import torch \n",
    "import transformers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb5H2WdfQzAM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nlg_eval_via_simi_measures.bary_score import BaryScoreMetric\n",
    "from nlg_eval_via_simi_measures.depth_score import DepthScoreMetric\n",
    "from nlg_eval_via_simi_measures.infolm import InfoLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjyPRpzhVnyX"
   },
   "source": [
    "# Testing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVK8xtYtVqFk",
    "outputId": "e45d5b89-104e-4a47-9606-3805dfb6425d"
   },
   "outputs": [],
   "source": [
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "# Baryscore\n",
    "\n",
    "def compute_bary(ref, hypothesis):\n",
    "  metric_call = BaryScoreMetric()\n",
    "  metric_call.prepare_idfs(ref, hypothesis)\n",
    "  final_preds = metric_call.evaluate_batch(ref, hypothesis)\n",
    "  print(final_preds)\n",
    "  print(\"=\"*25)\n",
    "\n",
    "compute_bary(ref, hypothesis)\n",
    "\n",
    "ref = ['I like my cakes very much']\n",
    "hypothesis = ['I like my cakes very much']\n",
    "\n",
    "compute_bary(ref, hypothesis)\n",
    "\n",
    "ref = ['I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much']\n",
    "\n",
    "compute_bary(ref, hypothesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPeCTkc9d3M6"
   },
   "source": [
    "*   W c'est pour distance de Wasserstein\n",
    "*   SD, c'est pour Sinkhorn Divergence, donc autre chose et a priori pas ce qui nous intéresse (pour différentes valeurs d'un paramètre)\n",
    "\n",
    "Donc, a priori, on prend juste \"baryscore_W\"\n",
    "\n",
    "* Il faut calculer les idf au niveau du corpus, mais d'après ce que je lis dans l'article, ca se fait séparemment pour chaque paire référence/candidat (en tout cas le contraire n'est pas clairement indiqué)\n",
    "\n",
    "Donc, à mon avis, même s'il y a une fonction \"evaluate_batch\", on doit faire les calculs individuellement pour chaque pair, et pas tout concaténer. Mais le calcul de l'idf doit se faire au niveau des **textes** complets que l'on compare. \n",
    "\n",
    "Plus la métrique est proche de 0, mieux c'est. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ft-H4fiVcTI8",
    "outputId": "5ac2343a-07d4-4a98-8756-fb056839e84e"
   },
   "outputs": [],
   "source": [
    "# DepthScore\n",
    "\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "metric_call = DepthScoreMetric()\n",
    "metric_call.prepare_idfs(ref, hypothesis)\n",
    "final_preds = metric_call.evaluate_batch(hypothesis,ref )\n",
    "print(\"DepthScore\")\n",
    "print(final_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dba27g1_hGFa"
   },
   "source": [
    "Calcul d'un embedding pour le candidat et la référence avec a single layer of Bert (comme Bertscore apparemment) puis calcule la divergence introduite dans \"a pseudo metric\", donc plus c'est proche de 0 et plus les deux phrases sont proches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iE5F3oygM5QV",
    "outputId": "0b571f2a-2849-46f9-f29a-90a203aa743f"
   },
   "outputs": [],
   "source": [
    "# Pour l'instant, ca fonctionne uniquement si on se met sur CPU (il manque surement un .todevice() quelque part)\n",
    "\n",
    "# InfoLM \n",
    "\n",
    "metric = InfoLM()\n",
    "\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "metric.prepare_idfs(ref, hypothesis)\n",
    "\n",
    "final_preds = metric.evaluate_batch(hypothesis, ref)#), \n",
    "                                    #idf_ref = metric.idf_dict_hyp,\n",
    "                                    #idf_hyps = metric.idf_dict_ref)\n",
    "                                    #idf_ref= idf_ref,\n",
    "                                    #idf_hyps= idf_hypot)\n",
    "\n",
    "#self.idf_dict_hyp, self.idf_dict_ref \n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2sbFBJ7iApZ"
   },
   "source": [
    "Encore une fois, il me semble qu'on calcule les idf à l'échelle d'une pair candidat (C) / référence (R). \n",
    "D'abord, séparemment pour C et R, le PMLM calcule une probabilité discrète par token en le masquant, puis on l'agrège en mettant des poids via les idf, puis on utilise une mesure de divergence pour estimer la distance entre les deux distributions de probabilité obtenues. \n",
    "\n",
    "Par défaut, on utilise Fisher Rao qui, d'après l'article, est pas mal et a l'avantage de ne pas inclure de paramètre à optimiser. \n",
    "\n",
    "On a 3 résultats, parce que l'algo permet d'utiliser des divergences, qui ne sont pas symétriques. Du coup le premier c'est Div(A,B), le second Div(B,A) et le 3ème la moyenne des deux. Mais vu qu'on utilise la DISTANCE de Fisher Rao (qui évidemment est une distance), elle est symétrique et osef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bleu by NLTK (sentence level)\n",
    "lambda_split_function=lambda x:  x.split()\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "ref_processed = list(map(lambda_split_function,ref))\n",
    "hyp_processed = list(map(lambda_split_function,hypothesis))\n",
    "\n",
    "for i in range(len(ref_processed)):\n",
    "    print(ref_processed[i],hyp_processed[i])\n",
    "    print(sentence_bleu([ref_processed[i]],hyp_processed[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bleu by NLTK (corpus level)\n",
    "lambda_split_function=lambda x:  x.split()\n",
    "ref = ['I like my cakes very much','I love these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I love my cakes very much']\n",
    "\n",
    "ref_processed = list(map(lambda_split_function,ref))\n",
    "hyp_processed = list(map(lambda_split_function,hypothesis))\n",
    "\n",
    "for i in range(len(hyp_processed)):\n",
    "    print(ref_processed,hyp_processed[i])\n",
    "    print(corpus_bleu([ref_processed],[hyp_processed[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# py-rouge\n",
    "\n",
    "import rouge\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "\n",
    "def prepare_results(m, p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(m, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "\n",
    "all_hypothesis = ['I like my cakes very much','I love my cakes very much']\n",
    "all_references = ['I like my cakes very much','I love these cakes!']\n",
    "\n",
    "for aggregator in ['Avg', 'Best', 'Individual']:\n",
    "    print('Evaluation with {}'.format(aggregator))\n",
    "    apply_avg = aggregator == 'Avg'\n",
    "    apply_best = aggregator == 'Best'\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n",
    "\n",
    "\n",
    "    scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + prepare_results(metric,results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print(prepare_results(metric, results['p'], results['r'], results['f']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/neural-dialogue-metrics/rouge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge import rouge_n_sentence_level\n",
    "\n",
    "\n",
    "lambda_split_function=lambda x:  x.split()\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "ref_processed = list(map(lambda_split_function,ref))\n",
    "hyp_processed = list(map(lambda_split_function,hypothesis))\n",
    "\n",
    "for i in range(len(ref_processed)):\n",
    "    reference_sentence=ref_processed[i]\n",
    "    summary_sentence=hyp_processed[i]\n",
    "\n",
    "    # Calculate ROUGE-2.\n",
    "    recall, precision, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)\n",
    "    print('ROUGE-2-R', recall)\n",
    "    print('ROUGE-2-P', precision)\n",
    "    print('ROUGE-2-F', rouge)\n",
    "\n",
    "    # If you just want the F-measure you can do this:\n",
    "    *_, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)  # Requires a Python-3 to use *_.\n",
    "    print('ROUGE-2-R', recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/pltrdy/rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pltrdy/rouge\n",
    "\n",
    "from rouge import Rouge \n",
    "\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, ref)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# METEOR score\n",
    "\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('wordnet')\n",
    "\n",
    "ref = ['I like my cakes very much','I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much','I like my cakes very much']\n",
    "\n",
    "for i in range(len(ref)):\n",
    "    wref=word_tokenize(ref[i])\n",
    "    whyp=word_tokenize(hypothesis[i])\n",
    "    print(wref,whyp)\n",
    "    print(round(meteor([wref],whyp), 4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
