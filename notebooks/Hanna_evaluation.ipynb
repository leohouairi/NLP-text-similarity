{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04b6b34-c474-4394-9585-85efe1f0e3fb",
   "metadata": {},
   "source": [
    "# Hanna evaluation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54395297-837a-4975-bc7a-fc9a80e04fb7",
   "metadata": {},
   "source": [
    "## 1 - Installation and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf3d35-58ca-4114-9c92-ee15b6dcad27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011988fe-c45c-460b-87c1-c8a3d69704e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1957d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers\n",
    "from nlg_eval_via_simi_measures.bary_score import BaryScoreMetric\n",
    "from nlg_eval_via_simi_measures.depth_score import DepthScoreMetric\n",
    "from nlg_eval_via_simi_measures.infolm import InfoLM\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from bert_score import score\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4f83c-10dd-4d64-8b3b-f149e085cf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download('punkt')\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf107e20-21b6-49b4-8d71-94469baea0d3",
   "metadata": {},
   "source": [
    "## 2 - Scores and correlations functions\n",
    "\n",
    "Where we define the function that we later be used to compute the scores and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd3020-a719-4e91-a430-e7ad068786d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BaryScore\n",
    "\n",
    "def compute_bary(ref:str,hypothesis:str):\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    metric_call = BaryScoreMetric()\n",
    "    metric_call.prepare_idfs(ref, hypothesis)\n",
    "    return metric_call.evaluate_batch(ref, hypothesis)[\"baryscore_W\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7134384-f210-4758-838e-572ec2270017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DepthScore\n",
    "\n",
    "def compute_depthscore(ref:str,hypothesis:str):\n",
    "    metric_call = DepthScoreMetric()\n",
    "    metric_call.prepare_idfs(ref, hypothesis)\n",
    "    return metric_call.evaluate_batch(hypothesis,ref)[\"depth_score\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0a529-ddc9-48c7-bf7c-916ba44fb4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# InfoLM\n",
    "\n",
    "def compute_infolmscore(ref:str,hypothesis:str):\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    metric = InfoLM()\n",
    "    #metric.device=\"cpu\"\n",
    "    #metric.model.to(\"cpu\")\n",
    "    metric.prepare_idfs(ref, hypothesis)\n",
    "    return metric.evaluate_batch(hypothesis, ref)[\"fisher_rao\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfee8f5-7836-4d4c-82d2-2c5a0ce35c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BLEU\n",
    "\n",
    "def compute_bleuscore(ref:str,hypothesis:str):\n",
    "    ref,hypothesis=[word_tokenize(ref)],word_tokenize(hypothesis)\n",
    "    return sentence_bleu(ref,hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6249e29-45c8-415b-ac9f-f05efa7e5956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BERTScore\n",
    "\n",
    "def compute_bertscore(ref:str,hypothesis:str):\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    P, R, F1=score(hypothesis, ref, lang=\"en\", verbose=True)\n",
    "    return P.item(),R.item(),F1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da6bbc-683d-4c09-ae87-c74c797c5cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BARTSCore function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BARTScorer:\n",
    "    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):\n",
    "        # Set up model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Set up loss\n",
    "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
    "        self.lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def load(self, path=None):\n",
    "        \"\"\" Load model from paraphrase finetuning \"\"\"\n",
    "        if path is None:\n",
    "            path = 'models/bart.pth'\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def score(self, srcs, tgts, batch_size=4):\n",
    "        \"\"\" Score a batch of examples \"\"\"\n",
    "        score_list = []\n",
    "        for i in range(0, len(srcs), batch_size):\n",
    "            src_list = srcs[i: i + batch_size]\n",
    "            tgt_list = tgts[i: i + batch_size]\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    encoded_src = self.tokenizer(\n",
    "                        src_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    encoded_tgt = self.tokenizer(\n",
    "                        tgt_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    src_tokens = encoded_src['input_ids'].to(self.device)\n",
    "                    src_mask = encoded_src['attention_mask'].to(self.device)\n",
    "\n",
    "                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)\n",
    "                    tgt_mask = encoded_tgt['attention_mask']\n",
    "                    tgt_len = tgt_mask.sum(dim=1).to(self.device)\n",
    "\n",
    "                    output = self.model(\n",
    "                        input_ids=src_tokens,\n",
    "                        attention_mask=src_mask,\n",
    "                        labels=tgt_tokens\n",
    "                    )\n",
    "                    logits = output.logits.view(-1, self.model.config.vocab_size)\n",
    "                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))\n",
    "                    loss = loss.view(tgt_tokens.shape[0], -1)\n",
    "                    loss = loss.sum(dim=1) / tgt_len\n",
    "                    curr_score_list = [-x.item() for x in loss]\n",
    "                    score_list += curr_score_list\n",
    "\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                print(f'source: {src_list}')\n",
    "                print(f'target: {tgt_list}')\n",
    "                exit(0)\n",
    "        return score_list\n",
    "\n",
    "    def multi_ref_score(self, srcs, tgts: List[List[str]], agg=\"mean\", batch_size=4):\n",
    "        # Assert we have the same number of references\n",
    "        ref_nums = [len(x) for x in tgts]\n",
    "        if len(set(ref_nums)) > 1:\n",
    "            raise Exception(\"You have different number of references per test sample.\")\n",
    "\n",
    "        ref_num = len(tgts[0])\n",
    "        score_matrix = []\n",
    "        for i in range(ref_num):\n",
    "            curr_tgts = [x[i] for x in tgts]\n",
    "            scores = self.score(srcs, curr_tgts, batch_size)\n",
    "            score_matrix.append(scores)\n",
    "        if agg == \"mean\":\n",
    "            score_list = np.mean(score_matrix, axis=0)\n",
    "        elif agg == \"max\":\n",
    "            score_list = np.max(score_matrix, axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return list(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd317f5-12dd-4fdf-bd45-279b9b9b21d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BARTSCore\n",
    "\n",
    "def compute_bartscore(ref:str,hypothesis:str):\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "    return bart_scorer.multi_ref_score(hypothesis, ref, agg=\"max\", batch_size=4)[0] # agg means aggregation, can be mean or max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57a54d-a283-4058-b974-2c9144fa6e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# METEOR\n",
    "\n",
    "def compute_meteorscore(ref:str,hypothesis:str):\n",
    "    return round(meteor([word_tokenize(ref)],word_tokenize(hypothesis)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053de45-af4f-4f3d-ad6f-c14d2768daf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ROUGE\n",
    "\n",
    "def id(r,p,f):\n",
    "    return r,p,f\n",
    "\n",
    "def compute_rougescore(ref:str,hypothesis:str):\n",
    "    rouge = Rouge() \n",
    "    return id(**rouge.get_scores([hypothesis], [ref])[0]['rouge-1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f52c036-a904-487d-929c-6345c5a7f4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions regarding the computation of correlations\n",
    "\n",
    "def compute_two_corr(dataframe:pd.DataFrame, column_name_1:str, column_name_2:str,correlation_type:str, system:str):\n",
    "    return dataframe[dataframe.Model==system][[column_name_1,column_name_2]].corr(method=correlation_type,numeric_only=True)\n",
    "\n",
    "def compute_list_corr(dataframe:pd.DataFrame, list_column_name:str, correlation_type:str, system:str=None): #list_column_name:list[str]\n",
    "    if system:\n",
    "        return dataframe[dataframe.Model==system][list_column_name].corr(method=correlation_type)\n",
    "    else:\n",
    "        return dataframe[list_column_name].corr(method=correlation_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08189d72-22a2-492b-b3da-2a45808fcdc0",
   "metadata": {},
   "source": [
    "## 3 - DL and datawrangling of Hanna dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "134d20bc-679e-4645-9703-754be60dc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset\n",
    "df = pd.read_csv(\"https://github.com/dig-team/hanna-benchmark-asg/raw/main/hanna_stories_annotations.csv\")\n",
    "\n",
    "# Selecting columns, dropping multiple lines corresponding to the human annotators\n",
    "df_unique_human_story=df[df.Model!=\"Human\"][[\"Story ID\",\"Human\",\"Story\",\"Model\"]].drop_duplicates(keep=\"first\")\n",
    "\n",
    "# Creation of an index\n",
    "df_human_index=df.Human.drop_duplicates(keep=\"first\").reset_index(drop=True).reset_index().rename(columns={\"index\":\"human_story_index\"})\n",
    "\n",
    "# Mean of the scores given by the human annotators\n",
    "df_mean_human_metrics=df.groupby(\"Story ID\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ee63d-a49d-42f0-acf9-7d8afc895fb2",
   "metadata": {},
   "source": [
    "## 4 - Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9821d1-3d72-40e7-a1a5-822b0796d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavy computation\n",
    "df_unique_human_story['baryscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bary(*x), axis =1)\n",
    "df_unique_human_story['depthscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_depthscore(*x), axis =1)\n",
    "df_unique_human_story['infolmscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_infolmscore(*x), axis =1)\n",
    "df_unique_human_story['BLEU']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bleuscore(*x), axis =1)\n",
    "df_unique_human_story[['ROUGE_r','ROUGE_p','ROUGE_f']]= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_rougescore(*x), axis =1, result_type=\"expand\")\n",
    "df_unique_human_story['meteorscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_meteorscore(*x), axis =1)\n",
    "df_unique_human_story['bartscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bartscore(*x), axis =1)\n",
    "df_unique_human_story[['bertscore_p','bertscore_r','bertscore_f1']]= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bertscore(*x), axis =1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7afaa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random numbers (for checking the rest of the code without doing the heavy computation)\n",
    "# df_unique_human_story['baryscore']= np.random.rand(960)\n",
    "# df_unique_human_story['depthscore']= np.random.rand(960)\n",
    "# df_unique_human_story['infolmscore']= np.random.rand(960)\n",
    "# df_unique_human_story['BLEU']= np.random.rand(960)\n",
    "# df_unique_human_story[['ROUGE_r','ROUGE_p','ROUGE_f']]= np.random.rand(960,3)\n",
    "# df_unique_human_story['meteorscore']= np.random.rand(960)\n",
    "# df_unique_human_story['bartscore']= np.random.rand(960)\n",
    "# df_unique_human_story[['bertscore_p','bertscore_r','bertscore_f1']]= np.random.rand(960,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3093ff61-3075-4ed6-b256-d67a0c356ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merging the computed metrix with the human scores\n",
    "df_unique_human_story_all=df_mean_human_metrics.merge(df_unique_human_story, how=\"left\", on=\"Story ID\")\n",
    "\n",
    "# Adding an index for the story (one per prompt)\n",
    "df_unique_human_story_all=df_unique_human_story_all.merge(df_human_index,on=\"Human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a59b285-1422-481b-9ec3-ec8c58b917f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unique_human_story_all.to_parquet(\"hanna_scores_computed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4874c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story ID</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Empathy</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Work time in seconds</th>\n",
       "      <th>Human</th>\n",
       "      <th>Story</th>\n",
       "      <th>...</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE_r</th>\n",
       "      <th>ROUGE_p</th>\n",
       "      <th>ROUGE_f</th>\n",
       "      <th>meteorscore</th>\n",
       "      <th>bartscore</th>\n",
       "      <th>bertscore_p</th>\n",
       "      <th>bertscore_r</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>human_story_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>161.333333</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>One morning, before you get up from bed you ch...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490226</td>\n",
       "      <td>0.489629</td>\n",
       "      <td>0.317254</td>\n",
       "      <td>0.675504</td>\n",
       "      <td>0.508353</td>\n",
       "      <td>0.884426</td>\n",
       "      <td>0.214604</td>\n",
       "      <td>0.449683</td>\n",
       "      <td>0.355625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>*Another ER trip, I’m going to need some paink...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631275</td>\n",
       "      <td>0.232564</td>\n",
       "      <td>0.143616</td>\n",
       "      <td>0.987630</td>\n",
       "      <td>0.538898</td>\n",
       "      <td>0.152707</td>\n",
       "      <td>0.419090</td>\n",
       "      <td>0.880116</td>\n",
       "      <td>0.291736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>204.666667</td>\n",
       "      <td>3,000 years have I been fighting. Every mornin...</td>\n",
       "      <td>it was hell. not exactly a place of torture. t...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>0.848330</td>\n",
       "      <td>0.043693</td>\n",
       "      <td>0.725099</td>\n",
       "      <td>0.071143</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.910074</td>\n",
       "      <td>0.782135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Story ID  Relevance  Coherence   Empathy  Surprise  Engagement  Complexity  \\\n",
       "0        96   1.666667   3.666667  2.666667  1.333333    2.666667    2.666667   \n",
       "1       192   2.333333   3.000000  2.333333  2.666667    2.666667    2.666667   \n",
       "2       288   3.666667   3.666667  3.000000  3.000000    3.000000    2.666667   \n",
       "\n",
       "   Work time in seconds                                              Human  \\\n",
       "0            161.333333  3,000 years have I been fighting. Every mornin...   \n",
       "1            248.000000  3,000 years have I been fighting. Every mornin...   \n",
       "2            204.666667  3,000 years have I been fighting. Every mornin...   \n",
       "\n",
       "                                               Story  ...      BLEU   ROUGE_r  \\\n",
       "0  One morning, before you get up from bed you ch...  ...  0.490226  0.489629   \n",
       "1  *Another ER trip, I’m going to need some paink...  ...  0.631275  0.232564   \n",
       "2  it was hell. not exactly a place of torture. t...  ...  0.019822  0.848330   \n",
       "\n",
       "    ROUGE_p   ROUGE_f  meteorscore  bartscore  bertscore_p  bertscore_r  \\\n",
       "0  0.317254  0.675504     0.508353   0.884426     0.214604     0.449683   \n",
       "1  0.143616  0.987630     0.538898   0.152707     0.419090     0.880116   \n",
       "2  0.043693  0.725099     0.071143   0.213745     0.002574     0.910074   \n",
       "\n",
       "   bertscore_f1  human_story_index  \n",
       "0      0.355625                  0  \n",
       "1      0.291736                  0  \n",
       "2      0.782135                  0  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dataframe\n",
    "df_unique_human_story_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ff404-05d8-45b0-94a1-12b820190bf3",
   "metadata": {},
   "source": [
    "## 5 - Compute correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d4f4a7f-c538-45d2-ad02-68e398f77a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIST_HUMAN_METRICS=['Relevance', 'Coherence', 'Empathy', 'Surprise', 'Engagement', 'Complexity']\n",
    "LIST_AEM=['baryscore', 'depthscore', 'infolmscore', 'BLEU', 'ROUGE_r', 'ROUGE_p', 'ROUGE_f', \n",
    "          'meteorscore', 'bartscore', 'bertscore_p', 'bertscore_r', 'bertscore_f1']\n",
    "LIST_ALL_METRICS=LIST_HUMAN_METRICS+LIST_AEM\n",
    "CORR_METHODS=[\"pearson\", \"kendall\", \"spearman\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a284a",
   "metadata": {},
   "source": [
    "### Text-level correlation\n",
    "The text-level correlation $C_{t,f}$ writes:\n",
    "$$C_{t,f} \\triangleq \\frac{1}{N}\\sum_{i=1}^N K(\\pmb{F}_i^t, \\pmb{H}_i^t)$$\n",
    "where $\\pmb{F}_i = [f(\\pmb{x_i},\\pmb{y_i^1}), ..., f(\\pmb{x_i},\\pmb{y_i^S})]$ and $\\pmb{H}_i = [h(\\pmb{x_i},\\pmb{y_i^1}), ..., h(\\pmb{x_i},\\pmb{y_i^S})]$ are the vectors composed of scores assigned by the automatic metric f and the human metric (h) respectively and $K: \\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow [-1,1]$ is the chosen correlation measure. [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bec13-e67f-4ff2-8269-9d04ba580c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for corr_method in CORR_METHODS:\n",
    "    print(corr_method)\n",
    "    # For each gold, compute AEM + HM (provided)\n",
    "\n",
    "    # >>> Done in  df_unique_human_story_all\n",
    "    df_work=df_unique_human_story_all.copy()\n",
    "    df_work=df_work.drop(columns=[\"Human\",\"Story\",\"Work time in seconds\",\"Story ID\"])\n",
    "    # For each gold (96), compute correlation 960 => 96 \n",
    "    list_of_dataframe=[]\n",
    "    for index in iter(df_work.human_story_index.unique()): # For each prompt ...\n",
    "        # ... compute the correlation between each pairs of columns\n",
    "        result=compute_list_corr(df_work[df_work[\"human_story_index\"]==index], LIST_ALL_METRICS, corr_method)\n",
    "        list_of_dataframe.append(result)\n",
    "\n",
    "    # For each pair of metrics, we then compute the average other the 96 prompts\n",
    "    # Take the mean sample\n",
    "    #sns.heatmap(pd.DataFrame(np.mean(list(map(lambda x : x.to_numpy(),list_of_dataframe)),axis=0),columns=LIST_ALL_METRICS))\n",
    "    sns.heatmap(pd.concat(list_of_dataframe).groupby(level=0).mean().reindex(LIST_ALL_METRICS),annot=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21aa9c",
   "metadata": {},
   "source": [
    "### System level correlation\n",
    "\n",
    "Similarly, the system level correlation $C_{sy,f}$ writes: \n",
    "$$C_{sy,f} \\triangleq K(\\pmb{F}^{sy}, \\pmb{H}^{sy})$$\n",
    "$$\\pmb{F}^{sy} = \\left[\\frac{1}{N}\\sum_{i=1}^Nf(\\pmb{x_i, y_i^1}), ...,  \\frac{1}{N}\\sum_{i=1}^Nf(\\pmb{x_i, y_i^S})\\right]$$\n",
    "$$\\pmb{H}^{sy} = \\left[\\frac{1}{N}\\sum_{i=1}^Nh(\\pmb{x_i, y_i^1}), ...,  \\frac{1}{N}\\sum_{i=1}^Nh(\\pmb{x_i, y_i^S})\\right]$$\n",
    "Where the latter are the vectors composed of the averaged scores assigned by the automatic metric f and the human annotation h. [1,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89406b20-b241-4e9b-8466-b9c106483e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for corr_method in CORR_METHODS:\n",
    "    print(corr_method)\n",
    "    # For each gold, compute AEM + HM (provided)\n",
    "\n",
    "    # >>> Done in  df_unique_human_story_all\n",
    "    df_work=df_unique_human_story_all.copy()\n",
    "    df_work=df_work.drop(columns=[\"Human\",\"Story\",\"Work time in seconds\",\"Story ID\"])\n",
    "    # Take the mean sample for each system 960 => 10\n",
    "    \n",
    "    # Mean for each score for each \"model\" ie for each ASG system\n",
    "    df_work.groupby(\"Model\").mean()\n",
    "\n",
    "    # Compute correlation\n",
    "    sns.heatmap(compute_list_corr(df_work,LIST_ALL_METRICS,corr_method), annot=True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e4b5f",
   "metadata": {},
   "source": [
    "# Sources\n",
    "Regarding the formalism of the two level of correelations. \n",
    "\n",
    "[1] Pierre Jean A Colombo, Chlo ́e Clavel, andPablo Piantanida. “Infolm: A new metric to evaluate summarization & data2text generation”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 10. 2022, pp. 10554–10562\n",
    "\n",
    "[2] Cyril Chhun et al. “Of human criteria and automatic metrics: A benchmark of the evaluation of story generation”. In: arXiv\n",
    "preprint arXiv:2208.11646 (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2218fb3-1a6b-45fb-9d53-8b48cc8700a9",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbe88c-d460-47a9-ad9c-6298f9d8d70c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mini=df_unique_human_story.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1091d33-591d-4f32-9aa1-9ba5242f52a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mini['baryscore']= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_bary(*x), axis =1)\n",
    "df_mini['depthscore']= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_depthscore(*x), axis =1)\n",
    "df_mini['infolmscore']= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_infolmscore(*x), axis =1)\n",
    "df_mini['BLEU']= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_bleuscore(*x), axis =1)\n",
    "df_mini[['ROUGE_r','ROUGE_p','ROUGE_f']]= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_rougescore(*x), axis =1, result_type=\"expand\")\n",
    "df_mini['meteorscore']= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_meteorscore(*x), axis =1)\n",
    "df_mini['bartscore']= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_bartscore(*x), axis =1)\n",
    "df_mini[['bertscore_p','bertscore_r','bertscore_f1']]= df_mini[[\"Human\", \"Story\"]].apply(lambda x : compute_bertscore(*x), axis =1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7611e8-2db3-487c-90ba-bd7408b132ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5ae3b-f949-4e4e-96bf-c846209ed3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unique_human_only=df[df.Model==\"Human\"][[\"Human\",\"Story\"]].drop_duplicates(keep=\"first\")\n",
    "df_mini_human=df_unique_human_only.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c72ef2-10f9-4298-bafd-3ff66561207f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mini_human['baryscore']= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_bary(*x), axis =1)\n",
    "df_mini_human['depthscore']= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_depthscore(*x), axis =1)\n",
    "df_mini_human['infolmscore']= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_infolmscore(*x), axis =1)\n",
    "df_mini_human['BLEU']= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_bleuscore(*x), axis =1)\n",
    "df_mini_human[['ROUGE_r','ROUGE_p','ROUGE_f']]= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_rougescore(*x), axis =1, result_type=\"expand\")\n",
    "df_mini_human['meteorscore']= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_meteorscore(*x), axis =1)\n",
    "df_mini_human['bartscore']= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_bartscore(*x), axis =1)\n",
    "df_mini_human[['bertscore_p','bertscore_r','bertscore_f1']]= df_mini_human[[\"Human\", \"Story\"]].apply(lambda x : compute_bertscore(*x), axis =1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9bfaa-fb0d-4d06-8a30-1db8d6e98645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mini_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30423a0-643e-4551-a35b-8563d524c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{‘pearson’, ‘kendall’, ‘spearman’}\n",
    "sns.heatmap(compute_list_corr(df_unique_human_story_all,LIST_ALL_METRICS,'spearman','Human'),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1db41-7ea7-4f90-a0a7-9d679a54a2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_two_corr(df_unique_human_story_all,'baryscore','Relevance','spearman','Human')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
