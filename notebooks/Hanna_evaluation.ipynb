{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04b6b34-c474-4394-9585-85efe1f0e3fb",
   "metadata": {},
   "source": [
    "# Hanna evaluation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a46d9-b101-4bdd-a2c9-1b02a31ab780",
   "metadata": {},
   "source": [
    "* notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3a5e7-5eaa-4a2e-a497-64b22b1dcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    COLAB_ENV=True\n",
    "else:\n",
    "    COLAB_ENV=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6cb6e-6fc2-462d-8711-8716001e66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODE=False\n",
    "SAVE_GRAPHICS=False\n",
    "COMPUTE_AND_SHOW_CROSS_CORR=True\n",
    "COMPUTE_AND_SHOW_BORDAS_COUNT=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54395297-837a-4975-bc7a-fc9a80e04fb7",
   "metadata": {},
   "source": [
    "## 1 - Installation and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf3d35-58ca-4114-9c92-ee15b6dcad27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COLAB_ENV:\n",
    "    # From Colab\n",
    "    %pip install -r https://raw.githubusercontent.com/leohouairi/NLP-text-similarity/main/requirements.txt\n",
    "else:\n",
    "    # From git clone\n",
    "    %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011988fe-c45c-460b-87c1-c8a3d69704e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1957d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers\n",
    "from nlg_eval_via_simi_measures.bary_score import BaryScoreMetric\n",
    "from nlg_eval_via_simi_measures.depth_score import DepthScoreMetric\n",
    "from nlg_eval_via_simi_measures.infolm import InfoLM\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from bert_score import score\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4f83c-10dd-4d64-8b3b-f149e085cf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download('punkt')\n",
    "download('wordnet')\n",
    "# Colab needs omw for Meteorscore\n",
    "if COLAB_ENV:\n",
    "    download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf107e20-21b6-49b4-8d71-94469baea0d3",
   "metadata": {},
   "source": [
    "## 2 - Scores and correlations functions\n",
    "\n",
    "Where we define the function that we later be used to compute the scores and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd3020-a719-4e91-a430-e7ad068786d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BaryScore\n",
    "\n",
    "def compute_bary(ref:str,hypothesis:str)-> float:\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    metric_call = BaryScoreMetric()\n",
    "    metric_call.prepare_idfs(ref, hypothesis)\n",
    "    return metric_call.evaluate_batch(ref, hypothesis)[\"baryscore_W\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7134384-f210-4758-838e-572ec2270017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DepthScore\n",
    "\n",
    "def compute_depthscore(ref:str,hypothesis:str)-> float:\n",
    "    metric_call = DepthScoreMetric()\n",
    "    metric_call.prepare_idfs(ref, hypothesis)\n",
    "    return metric_call.evaluate_batch(hypothesis,ref)[\"depth_score\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0a529-ddc9-48c7-bf7c-916ba44fb4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# InfoLM\n",
    "\n",
    "def compute_infolmscore(ref:str,hypothesis:str)-> float:\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    metric = InfoLM()\n",
    "    #metric.device=\"cpu\"\n",
    "    #metric.model.to(\"cpu\")\n",
    "    metric.prepare_idfs(ref, hypothesis)\n",
    "    return metric.evaluate_batch(hypothesis, ref)[\"fisher_rao\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfee8f5-7836-4d4c-82d2-2c5a0ce35c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BLEU\n",
    "\n",
    "def compute_bleuscore(ref:str,hypothesis:str)-> float:\n",
    "    ref,hypothesis=[word_tokenize(ref)],word_tokenize(hypothesis)\n",
    "    return sentence_bleu(ref,hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6249e29-45c8-415b-ac9f-f05efa7e5956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BERTScore\n",
    "\n",
    "def compute_bertscore(ref:str,hypothesis:str)-> tuple[float]:\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    P, R, F1=score(hypothesis, ref, lang=\"en\", verbose=True)\n",
    "    return P.item(),R.item(),F1.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ec9be-70d9-4dc1-9e5f-5b9e93e3399f",
   "metadata": {},
   "source": [
    "* Code from https://github.com/neulab/BARTScore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da6bbc-683d-4c09-ae87-c74c797c5cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BARTSCore function code from https://github.com/neulab/BARTScore \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BARTScorer:\n",
    "    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):\n",
    "        # Set up model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Set up loss\n",
    "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
    "        self.lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def load(self, path=None):\n",
    "        \"\"\" Load model from paraphrase finetuning \"\"\"\n",
    "        if path is None:\n",
    "            path = 'models/bart.pth'\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def score(self, srcs, tgts, batch_size=4):\n",
    "        \"\"\" Score a batch of examples \"\"\"\n",
    "        score_list = []\n",
    "        for i in range(0, len(srcs), batch_size):\n",
    "            src_list = srcs[i: i + batch_size]\n",
    "            tgt_list = tgts[i: i + batch_size]\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    encoded_src = self.tokenizer(\n",
    "                        src_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    encoded_tgt = self.tokenizer(\n",
    "                        tgt_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    src_tokens = encoded_src['input_ids'].to(self.device)\n",
    "                    src_mask = encoded_src['attention_mask'].to(self.device)\n",
    "\n",
    "                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)\n",
    "                    tgt_mask = encoded_tgt['attention_mask']\n",
    "                    tgt_len = tgt_mask.sum(dim=1).to(self.device)\n",
    "\n",
    "                    output = self.model(\n",
    "                        input_ids=src_tokens,\n",
    "                        attention_mask=src_mask,\n",
    "                        labels=tgt_tokens\n",
    "                    )\n",
    "                    logits = output.logits.view(-1, self.model.config.vocab_size)\n",
    "                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))\n",
    "                    loss = loss.view(tgt_tokens.shape[0], -1)\n",
    "                    loss = loss.sum(dim=1) / tgt_len\n",
    "                    curr_score_list = [-x.item() for x in loss]\n",
    "                    score_list += curr_score_list\n",
    "\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                print(f'source: {src_list}')\n",
    "                print(f'target: {tgt_list}')\n",
    "                exit(0)\n",
    "        return score_list\n",
    "\n",
    "    def multi_ref_score(self, srcs, tgts: List[List[str]], agg=\"mean\", batch_size=4):\n",
    "        # Assert we have the same number of references\n",
    "        ref_nums = [len(x) for x in tgts]\n",
    "        if len(set(ref_nums)) > 1:\n",
    "            raise Exception(\"You have different number of references per test sample.\")\n",
    "\n",
    "        ref_num = len(tgts[0])\n",
    "        score_matrix = []\n",
    "        for i in range(ref_num):\n",
    "            curr_tgts = [x[i] for x in tgts]\n",
    "            scores = self.score(srcs, curr_tgts, batch_size)\n",
    "            score_matrix.append(scores)\n",
    "        if agg == \"mean\":\n",
    "            score_list = np.mean(score_matrix, axis=0)\n",
    "        elif agg == \"max\":\n",
    "            score_list = np.max(score_matrix, axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return list(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd317f5-12dd-4fdf-bd45-279b9b9b21d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BARTSCore\n",
    "\n",
    "def compute_bartscore(ref:str,hypothesis:str)-> float:\n",
    "    ref,hypothesis=[ref],[hypothesis]\n",
    "    bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "    return bart_scorer.multi_ref_score(hypothesis, ref, agg=\"max\", batch_size=4)[0] # agg means aggregation, can be mean or max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57a54d-a283-4058-b974-2c9144fa6e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# METEOR\n",
    "\n",
    "def compute_meteorscore(ref:str,hypothesis:str)-> float:\n",
    "    return round(meteor([word_tokenize(ref)],word_tokenize(hypothesis)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053de45-af4f-4f3d-ad6f-c14d2768daf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ROUGE\n",
    "\n",
    "def id(r,p,f):\n",
    "    return r,p,f\n",
    "\n",
    "def compute_rougescore(ref:str,hypothesis:str)-> tuple[float]:\n",
    "    rouge = Rouge() \n",
    "    return id(**rouge.get_scores([hypothesis], [ref])[0]['rouge-1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52c036-a904-487d-929c-6345c5a7f4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions regarding the computation of correlations\n",
    "\n",
    "def compute_two_corr(dataframe:pd.DataFrame, column_name_1:str, column_name_2:str,correlation_type:str, system:str)-> pd.DataFrame:\n",
    "    return dataframe[dataframe.Model==system][[column_name_1,column_name_2]].corr(method=correlation_type,numeric_only=True)\n",
    "\n",
    "def compute_list_corr(dataframe:pd.DataFrame, list_column_name:str, correlation_type:str, system:str=None)-> pd.DataFrame: #list_column_name:list[str]\n",
    "    if system:\n",
    "        return dataframe[dataframe.Model==system][list_column_name].corr(method=correlation_type)\n",
    "    else:\n",
    "        return dataframe[list_column_name].corr(method=correlation_type)\n",
    "\n",
    "# Transforms a correlation matrix: absolute value (since we are only interested in the strengh of the association), percentage\n",
    "\n",
    "def format_corr(corr)-> pd.DataFrame:\n",
    "    return 100*np.abs(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08189d72-22a2-492b-b3da-2a45808fcdc0",
   "metadata": {},
   "source": [
    "## 3 - DL and datawrangling of Hanna dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d20bc-679e-4645-9703-754be60dc5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importation of the dataset\n",
    "df = pd.read_csv(\"https://github.com/dig-team/hanna-benchmark-asg/raw/main/hanna_stories_annotations.csv\")\n",
    "\n",
    "# Selecting columns, dropping multiple lines corresponding to the human annotators\n",
    "df_unique_human_story=df[df.Model!=\"Human\"][[\"Story ID\",\"Human\",\"Story\",\"Model\"]].drop_duplicates(keep=\"first\")\n",
    "\n",
    "# Creation of an index\n",
    "df_human_index=df.Human.drop_duplicates(keep=\"first\").reset_index(drop=True).reset_index().rename(columns={\"index\":\"human_story_index\"})\n",
    "\n",
    "# Mean of the scores given by the human annotators\n",
    "df_mean_human_metrics=df.groupby(\"Story ID\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ee63d-a49d-42f0-acf9-7d8afc895fb2",
   "metadata": {},
   "source": [
    "## 4 - Compute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef0f8a5-eaee-4e3c-9790-3c2041d09fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    # Random numbers (for checking the rest of the code without doing the heavy computation)\n",
    "    row_count,_=df_unique_human_story.shape\n",
    "    df_unique_human_story['baryscore']= np.random.rand(row_count)\n",
    "    df_unique_human_story['depthscore']= np.random.rand(row_count)\n",
    "    df_unique_human_story['infolmscore']= np.random.rand(row_count)\n",
    "    df_unique_human_story['BLEU']= np.random.rand(row_count)\n",
    "    df_unique_human_story[['ROUGE_r','ROUGE_p','ROUGE_f']]= np.random.rand(row_count,3)\n",
    "    df_unique_human_story['meteorscore']= np.random.rand(row_count)\n",
    "    df_unique_human_story['bartscore']= np.random.rand(row_count)\n",
    "    df_unique_human_story[['bertscore_p','bertscore_r','bertscore_f1']]= np.random.rand(row_count,3)\n",
    "else:\n",
    "    # Heavy computation\n",
    "    df_unique_human_story['baryscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bary(*x), axis =1)\n",
    "    df_unique_human_story['depthscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_depthscore(*x), axis =1)\n",
    "    df_unique_human_story['infolmscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_infolmscore(*x), axis =1)\n",
    "    df_unique_human_story['BLEU']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bleuscore(*x), axis =1)\n",
    "    df_unique_human_story[['ROUGE_r','ROUGE_p','ROUGE_f']]= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_rougescore(*x), axis =1, result_type=\"expand\")\n",
    "    df_unique_human_story['meteorscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_meteorscore(*x), axis =1)\n",
    "    df_unique_human_story['bartscore']= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bartscore(*x), axis =1)\n",
    "    df_unique_human_story[['bertscore_p','bertscore_r','bertscore_f1']]= df_unique_human_story[[\"Human\", \"Story\"]].apply(lambda x : compute_bertscore(*x), axis =1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093ff61-3075-4ed6-b256-d67a0c356ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merging the computed metrix with the human scores\n",
    "df_unique_human_story_all=df_mean_human_metrics.merge(df_unique_human_story, how=\"left\", on=\"Story ID\")\n",
    "\n",
    "# Adding an index for the story (one per prompt)\n",
    "df_unique_human_story_all=df_unique_human_story_all.merge(df_human_index,on=\"Human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59b285-1422-481b-9ec3-ec8c58b917f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unique_human_story_all.to_parquet(\"hanna_scores_computed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4874c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataframe\n",
    "df_unique_human_story_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ff404-05d8-45b0-94a1-12b820190bf3",
   "metadata": {},
   "source": [
    "## 5 - Compute correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f4a7f-c538-45d2-ad02-68e398f77a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIST_HUMAN_METRICS=['Relevance', 'Coherence', 'Empathy', 'Surprise', 'Engagement', 'Complexity']\n",
    "LIST_AEM=['baryscore', 'depthscore', 'infolmscore', 'BLEU', 'ROUGE_r', 'ROUGE_p', 'ROUGE_f', \n",
    "          'meteorscore', 'bartscore', 'bertscore_p', 'bertscore_r', 'bertscore_f1']\n",
    "LIST_ALL_METRICS=LIST_HUMAN_METRICS+LIST_AEM\n",
    "CORR_METHODS=[\"pearson\", \"kendall\", \"spearman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4c1a8-2e4f-41ff-985b-7eb5c0ce4826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the file containing the result of the heavy coputations\n",
    "df_unique_human_story_all=pd.read_parquet(\"hanna_scores_computed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4e13b",
   "metadata": {},
   "source": [
    "**Important note**. In the following cells, saving of the figures is not active. If you want to save them, please put SAVE_GRAPHICS to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a284a",
   "metadata": {},
   "source": [
    "### Text-level correlation\n",
    "The text-level correlation $C_{t,f}$ writes:\n",
    "$$C_{t,f} \\triangleq \\frac{1}{N}\\sum_{i=1}^N K(\\pmb{F}_i^t, \\pmb{H}_i^t)$$\n",
    "where $\\pmb{F}_i = [f(\\pmb{x_i},\\pmb{y_i^1}), ..., f(\\pmb{x_i},\\pmb{y_i^S})]$ and $\\pmb{H}_i = [h(\\pmb{x_i},\\pmb{y_i^1}), ..., h(\\pmb{x_i},\\pmb{y_i^S})]$ are the vectors composed of scores assigned by the automatic metric f and the human metric (h) respectively and $K: \\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow [-1,1]$ is the chosen correlation measure. [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bec13-e67f-4ff2-8269-9d04ba580c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"TEXT LEVEL\")\n",
    "\n",
    "list_score = []\n",
    "list_ranking = [] \n",
    "\n",
    "for corr_method in CORR_METHODS:\n",
    "    print(corr_method)\n",
    "    # For each gold, compute AEM + HM (provided)\n",
    "\n",
    "    df_work=df_unique_human_story_all.copy()\n",
    "    df_work=df_work.drop(columns=[\"Human\",\"Story\",\"Work time in seconds\",\"Story ID\"])\n",
    "    # For each gold (96), compute correlation 960 => 96 \n",
    "    list_of_dataframe=[]\n",
    "    for index in iter(df_work.human_story_index.unique()): # For each prompt ...\n",
    "        # ... compute the correlation between each pairs of columns\n",
    "        result=compute_list_corr(df_work[df_work[\"human_story_index\"]==index], LIST_ALL_METRICS, corr_method)\n",
    "        list_of_dataframe.append(result)\n",
    "\n",
    "    # For each pair of metrics, we then compute the average other the 96 prompts\n",
    "    # Take the mean sample\n",
    "    sns.heatmap(pd.DataFrame(np.mean(list(map(lambda x : x.to_numpy(),list_of_dataframe)),axis=0),columns=LIST_ALL_METRICS))\n",
    "    all_corr=pd.concat(list_of_dataframe).groupby(level=0).mean().reindex(LIST_ALL_METRICS)\n",
    "    \n",
    "    if COMPUTE_AND_SHOW_CROSS_CORR:\n",
    "        corr=all_corr.loc[LIST_HUMAN_METRICS,LIST_HUMAN_METRICS]\n",
    "        matrix = np.triu(corr)\n",
    "        sns.heatmap(format_corr(corr),mask=matrix, annot=True, fmt=\".0f\", cbar = False)\n",
    "        if SAVE_GRAPHICS:\n",
    "            plt.savefig(\"figures/text-\"+corr_method+\"-human\", bbox_inches='tight', dpi =300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        corr=all_corr.loc[LIST_AEM,LIST_AEM]\n",
    "        matrix = np.triu(corr)\n",
    "        sns.heatmap(format_corr(corr), mask=matrix,annot=True, fmt=\".0f\", cbar = False)\n",
    "        if SAVE_GRAPHICS:\n",
    "            plt.savefig(\"figures/text-\"+corr_method+\"-auto\", bbox_inches='tight', dpi =300)\n",
    "        plt.show()\n",
    "\n",
    "        corr=all_corr.loc[LIST_HUMAN_METRICS,LIST_AEM]\n",
    "        sns.heatmap(format_corr(corr),annot=True, fmt=\".0f\", cbar = False)\n",
    "        if SAVE_GRAPHICS:\n",
    "            plt.savefig(\"figures/text-\"+corr_method+\"-autohuman\", bbox_inches='tight', dpi =300)\n",
    "        plt.show()\n",
    "    \n",
    "    if COMPUTE_AND_SHOW_BORDAS_COUNT:\n",
    "        # Compute Borda\n",
    "        corr_b = format_corr(all_corr.loc[LIST_AEM, LIST_HUMAN_METRICS]) # Involves taking the absolute value\n",
    "        # Be wary: we want to rank in descending order, since the best AEM is the one associated with the highest correlation\n",
    "        corr_b = corr_b.rank(axis = 0, ascending = False)\n",
    "\n",
    "        cname = \"Ranking-\"+corr_method\n",
    "\n",
    "        # Summing the ranks\n",
    "        corr_b[cname] = corr_b.sum(axis = 1)\n",
    "        # Sorting the sum\n",
    "        corr_b = corr_b.sort_values(cname)\n",
    "\n",
    "        # Appending the sum of ranks \n",
    "        list_score.append(corr_b[cname])\n",
    "        # Appending the rank\n",
    "        list_ranking.append(corr_b[cname].rank())\n",
    "    \n",
    "\n",
    "if COMPUTE_AND_SHOW_BORDAS_COUNT:\n",
    "    data_score = pd.concat(list_score, axis = 1)\n",
    "    data_ranking = pd.concat(list_ranking, axis = 1)\n",
    "\n",
    "    print(data_score.sort_values(\"Ranking-kendall\"))\n",
    "    print(data_ranking.sort_values(\"Ranking-kendall\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21aa9c",
   "metadata": {},
   "source": [
    "### System level correlation\n",
    "\n",
    "Similarly, the system level correlation $C_{sy,f}$ writes: \n",
    "$$C_{sy,f} \\triangleq K(\\pmb{F}^{sy}, \\pmb{H}^{sy})$$\n",
    "$$\\pmb{F}^{sy} = \\left[\\frac{1}{N}\\sum_{i=1}^Nf(\\pmb{x_i, y_i^1}), ...,  \\frac{1}{N}\\sum_{i=1}^Nf(\\pmb{x_i, y_i^S})\\right]$$\n",
    "$$\\pmb{H}^{sy} = \\left[\\frac{1}{N}\\sum_{i=1}^Nh(\\pmb{x_i, y_i^1}), ...,  \\frac{1}{N}\\sum_{i=1}^Nh(\\pmb{x_i, y_i^S})\\right]$$\n",
    "Where the latter are the vectors composed of the averaged scores assigned by the automatic metric f and the human annotation h. [1,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89406b20-b241-4e9b-8466-b9c106483e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"SYSTEM LEVEL\")\n",
    "list_score = []\n",
    "list_ranking = [] \n",
    "\n",
    "for corr_method in CORR_METHODS:\n",
    "    print(corr_method)\n",
    "    # For each gold, compute AEM + HM (provided)\n",
    "\n",
    "    df_work=df_unique_human_story_all.copy()\n",
    "    df_work=df_work.drop(columns=[\"Human\",\"Story\",\"Work time in seconds\",\"Story ID\"])\n",
    "    # Take the mean sample for each system 960 => 10\n",
    "    \n",
    "    # Mean for each score for each \"model\" ie for each ASG system\n",
    "    df_work = df_work.groupby(\"Model\").mean()  \n",
    "\n",
    "    # Compute correlation\n",
    "    all_corr=compute_list_corr(df_work,LIST_ALL_METRICS,corr_method)\n",
    "    \n",
    "    if COMPUTE_AND_SHOW_CROSS_CORR:\n",
    "        corr=all_corr.loc[LIST_HUMAN_METRICS,LIST_HUMAN_METRICS]\n",
    "        matrix = np.triu(corr)\n",
    "\n",
    "        sns.heatmap(format_corr(corr),mask=matrix,annot=True, fmt=\".0f\", cbar = False)\n",
    "        if SAVE_GRAPHICS:\n",
    "            plt.savefig(\"figures/system-\"+corr_method+\"-human\", bbox_inches='tight', dpi =300)\n",
    "        plt.show()\n",
    "\n",
    "        corr=all_corr.loc[LIST_AEM,LIST_AEM]\n",
    "        matrix = np.triu(corr)\n",
    "        # We take the absolute value since we are only interested in the assocaition strength\n",
    "        sns.heatmap(format_corr(corr),mask=matrix,annot=True, fmt=\".0f\", cbar = False)\n",
    "        if SAVE_GRAPHICS:\n",
    "            plt.savefig(\"figures/system-\"+corr_method+\"-auto\", bbox_inches='tight', dpi =300)\n",
    "        plt.show()\n",
    "\n",
    "        corr=all_corr.loc[LIST_HUMAN_METRICS,LIST_AEM]\n",
    "        sns.heatmap(format_corr(corr),annot=True, fmt=\".0f\", cbar = False)\n",
    "        if SAVE_GRAPHICS:\n",
    "            plt.savefig(\"figures/system-\"+corr_method+\"-autohuman\", bbox_inches='tight', dpi =300)\n",
    "        plt.show()\n",
    "    \n",
    "    if COMPUTE_AND_SHOW_BORDAS_COUNT:\n",
    "        # Compute Borda\n",
    "        corr_b = format_corr(all_corr.loc[LIST_AEM, LIST_HUMAN_METRICS]) # Involves taking the absolute value\n",
    "        # Be wary: we want to rank in descending order, since the best AEM is the one associated with the highest correlation\n",
    "        corr_b = corr_b.rank(axis = 0, ascending = False)\n",
    "\n",
    "        cname = \"Ranking-\"+corr_method\n",
    "\n",
    "        # Summing the ranks \n",
    "        corr_b[cname] = corr_b.sum(axis = 1)\n",
    "        # Ranking the sum\n",
    "        corr_b = corr_b.sort_values(cname)\n",
    "\n",
    "        # Appending the sum of ranks\n",
    "        list_score.append(corr_b[cname])\n",
    "        # Appending the rank\n",
    "        list_ranking.append(corr_b[cname].rank())\n",
    "    \n",
    "\n",
    "if COMPUTE_AND_SHOW_BORDAS_COUNT:\n",
    "    data_score = pd.concat(list_score, axis = 1)\n",
    "    data_ranking = pd.concat(list_ranking, axis = 1)\n",
    "\n",
    "    print(data_score.sort_values(\"Ranking-kendall\"))\n",
    "    print(data_ranking.sort_values(\"Ranking-kendall\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e4b5f",
   "metadata": {},
   "source": [
    "# Sources\n",
    "Regarding the formalism of the two level of correlations. \n",
    "\n",
    "[1] Pierre Jean A Colombo, Chlo ́e Clavel, andPablo Piantanida. “Infolm: A new metric to evaluate summarization & data2text generation”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 10. 2022, pp. 10554–10562\n",
    "\n",
    "[2] Cyril Chhun et al. “Of human criteria and automatic metrics: A benchmark of the evaluation of story generation”. In: arXiv\n",
    "preprint arXiv:2208.11646 (2022)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
